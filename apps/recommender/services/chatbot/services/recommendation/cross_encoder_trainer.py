import torch
import torch.nn as nn
from transformers import RobertaForSequenceClassification, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
import torch.cuda.amp as amp
import pandas as pd
import numpy as np
from scipy.stats import spearmanr
from tqdm import tqdm
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax
import sys
import random


# 1. Pointwise í•™ìŠµ/í‰ê°€ìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤ -------------------------
# (ê¸°ì¡´ KReRankingEvalDatasetê³¼ ë™ì¼ êµ¬ì¡°, í•™ìŠµ/í‰ê°€ì— ëª¨ë‘ ì‚¬ìš©)
class PointwiseRankingDataset(Dataset):
    """
    Pointwise í•™ìŠµê³¼ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹.
    ê° (query, item, score) ìŒì„ ê°œë³„ ë°ì´í„°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, data_path, tokenizer, max_length=256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            df = pd.read_csv(data_path, sep='\t', on_bad_lines='warn')
        except FileNotFoundError:
            print(f"ê²½ê³ : {data_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë°ì´í„°ì…‹ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            self.queries, self.items, self.scores = [], [], []
            return

        self.queries = df['query'].astype(str).tolist()
        self.items = df['item_text'].astype(str).tolist()
        self.scores = df['score'].tolist()

    def __len__(self):
        return len(self.queries)

    def __getitem__(self, idx):
        features = self.tokenizer(
            self.queries[idx], self.items[idx],
            padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'
        )
        return {
            'input_ids': features['input_ids'].squeeze(0),
            'attention_mask': features['attention_mask'].squeeze(0),
            'labels': torch.tensor(self.scores[idx], dtype=torch.float)
        }


# 2. íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ (MSELoss ì ìš©) --------------------------------
import random

class CrossEncoderFineTuner:
    def __init__(self, model_name="klue/roberta-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=1)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.scaler = amp.GradScaler()  # AMP ìŠ¤ì¼€ì¼ëŸ¬ ì¶”ê°€
        print(f"âœ… í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device})")

    def smooth_labels(self, labels, epsilon=0.02):
        # ë¼ë²¨ì„ -epsilon~+epsilon ë²”ìœ„ì—ì„œ ê· ë“± ë¶„í¬ë¡œ ì‚´ì§ í”ë“¤ê¸°
        noise = (torch.rand_like(labels) * 2 - 1) * epsilon
        smoothed = labels + noise
        # 0~1 ë²”ìœ„ë¡œ í´ë¨í”„
        return torch.clamp(smoothed, 0.0, 1.0)

    def train(self, train_path, dev_path, output_path, batch_size=32, epochs=5, lr=2e-5, warmup_ratio=0.1, patience=2):
        print("\n--- ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œì‘ ---")
        train_set = PointwiseRankingDataset(train_path, self.tokenizer)
        dev_set = PointwiseRankingDataset(dev_path, self.tokenizer)

        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
        dev_loader = DataLoader(dev_set, batch_size=batch_size * 2)
        print("--- ë°ì´í„° ë¡œë”© ì™„ë£Œ ---\n")

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)
        loss_fn = nn.MSELoss()

        num_training_steps = len(train_loader) * epochs
        num_warmup_steps = int(num_training_steps * warmup_ratio)
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
        )

        best_score = -1.0
        epochs_no_improve = 0

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} (Training)", leave=True)

            for batch in progress_bar:
                optimizer.zero_grad()

                b_input_ids = batch['input_ids'].to(self.device)
                b_attention_mask = batch['attention_mask'].to(self.device)
                b_labels = batch['labels'].to(self.device)

                smoothed_labels = self.smooth_labels(b_labels)  # label smoothing ì ìš©

                with amp.autocast():
                    outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_mask)
                    loss = loss_fn(outputs.logits.squeeze(), smoothed_labels)

                total_loss += loss.item()

                if torch.isnan(loss):
                    print("ğŸš¨ NaN loss ë°œìƒ! logits:", outputs.logits, "labels:", b_labels)

                self.scaler.scale(loss).backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.scaler.step(optimizer)
                self.scaler.update()
                scheduler.step()

                progress_bar.set_postfix({'loss': loss.item()})

            avg_train_loss = total_loss / len(train_loader)

            # --- í‰ê°€ ---
            self.model.eval()
            all_preds, all_labels = [], []
            eval_progress_bar = tqdm(dev_loader, desc=f"Epoch {epoch + 1}/{epochs} (Validation)", leave=True)
            with torch.no_grad():
                for batch in eval_progress_bar:
                    inputs = {k: v.to(self.device) for k, v in batch.items() if k != 'labels'}
                    labels = batch['labels']
                    outputs = self.model(**inputs)
                    all_preds.extend(np.atleast_1d(outputs.logits.squeeze().cpu().numpy()))
                    all_labels.extend(np.atleast_1d(labels.numpy()))

            spearman_corr, _ = spearmanr(all_labels, all_preds)
            print(f"\nEpoch {epoch + 1} ê²°ê³¼ | í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f} | ê²€ì¦ Spearman ìƒê´€ê³„ìˆ˜: {spearman_corr:.4f}")

            if spearman_corr > best_score:
                best_score = spearman_corr
                epochs_no_improve = 0
                print(f"âœ¨ ìµœê³  ì ìˆ˜ ê°±ì‹ ! ëª¨ë¸ì„ '{output_path}'ì— ì €ì¥í•©ë‹ˆë‹¤. (ìƒê´€ê³„ìˆ˜: {best_score:.4f})")
                os.makedirs(output_path, exist_ok=True)
                self.model.save_pretrained(output_path)
                self.tokenizer.save_pretrained(output_path)
            else:
                epochs_no_improve += 1
                print(f"ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ. ({epochs_no_improve}/{patience})")

            if epochs_no_improve >= patience:
                print(f"\nâš ï¸ {patience}ë²ˆì˜ Epoch ë™ì•ˆ ì„±ëŠ¥ í–¥ìƒì´ ì—†ì–´ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            print("-" * 70)

        print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ. ìµœì¢… ìµœê³  ì ìˆ˜: {best_score:.4f}")


# 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ----------------------------------------
if __name__ == '__main__':
    # --- âš™ï¸ í•™ìŠµ ì„¤ì • ---
    TRAIN_FILE = 'cross_encoder_train_fixed.tsv'
    DEV_FILE = 'cross_encoder_dev_fixed.tsv'
    OUTPUT_MODEL_PATH = './my_best_reranker_mse'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë³€ê²½

    EPOCHS = 10
    # Pointwise í•™ìŠµì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ì¢‹ìœ¼ë¯€ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    BATCH_SIZE = 32
    LEARNING_RATE = 3e-5
    PATIENCE = 3  # ê²€ì¦ ì ìˆ˜ê°€ 2ë²ˆ ì—°ì† í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨

    # --- ğŸš€ í•™ìŠµ ì‹œì‘ ---
    print("--- ì¬ë­í‚¹ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (MSE Loss) ---")
    print(f"í•™ìŠµ ë°ì´í„°: {TRAIN_FILE}")
    print(f"ê²€ì¦ ë°ì´í„°: {DEV_FILE}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {OUTPUT_MODEL_PATH}")
    print(f"Batch Size: {BATCH_SIZE}, Patience: {PATIENCE}")
    print("-" * 40)

    fine_tuner = CrossEncoderFineTuner()
    fine_tuner.train(
        train_path=TRAIN_FILE,
        dev_path=DEV_FILE,
        output_path=OUTPUT_MODEL_PATH,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        lr=LEARNING_RATE,
        patience=PATIENCE,
    )

# ì¶”ë¡ (ì¬ë­í‚¹) í´ë˜ìŠ¤ --------------------------


class KCrossEncoderReranker:
    def __init__(self, model_path, device=None, max_length=256, normalize_scores=True):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.eval()

        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.max_length = max_length
        self.normalize_scores = normalize_scores  # ğŸ‘‰ softmax ì •ê·œí™” ì—¬ë¶€

    def rerank(self, query, candidates, top_n=5):
        # ê° í›„ë³´ì— ëŒ€í•´ title, overview, intro ê²°í•©
        item_texts = [
            f"{item.get('title', '')} {item.get('overview', '')} {item.get('intro', '')}".strip()
            for item in candidates
        ]
        pairs = [(query, text) for text in item_texts]

        # tokenizerë¡œ ì¸ì½”ë”©
        encodings = self.tokenizer.batch_encode_plus(
            pairs,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        encodings = {k: v.to(self.device) for k, v in encodings.items()}

        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**encodings)
            logits = outputs.logits.squeeze()

            if logits.dim() == 0:  # ì…ë ¥ì´ í•˜ë‚˜ì¼ ê²½ìš°ì—ë„ ëŒ€ì‘
                scores = logits.unsqueeze(0).cpu().numpy()
            else:
                scores = logits.cpu().numpy()

            if self.normalize_scores:
                scores = softmax(torch.tensor(scores), dim=0).numpy()

        sorted_indices = np.argsort(scores)[::-1]
        return [candidates[i] for i in sorted_indices[:top_n]]
