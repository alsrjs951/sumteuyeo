'''from sentence_transformers import SentenceTransformer
import numpy as np
import json
from typing import Dict, List, Any, Optional, Tuple
from openai import OpenAI
from dotenv import load_dotenv
import os
from datetime import datetime
import time
from constants import cat_dict

print("SentenceTransformer ëª¨ë¸(snunlp/KR-SBERT-V40K-klueNLI-augSTS)ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

print(".env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
load_dotenv()
print("í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì‹œë„ ì™„ë£Œ.")

print("OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
client = OpenAI()
print("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")

# GPT API í˜¸ì¶œ ë°°ì¹˜ í¬ê¸° (í•œ ë²ˆì— ëª‡ ê°œë¥¼ ìš”ì•½í• ì§€)
# ì´ ê°’ì€ ì‹¤í—˜ì„ í†µí•´ ìµœì ê°’ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
# ë„ˆë¬´ í¬ë©´ í† í° ì œí•œì— ê±¸ë¦¬ê±°ë‚˜ API ì‘ë‹µì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ë„ˆë¬´ ì‘ìœ¼ë©´ API í˜¸ì¶œ íšŸìˆ˜ê°€ ë§ì•„ì§‘ë‹ˆë‹¤.
GPT_BATCH_SIZE = 4  # ì˜ˆì‹œ: í•œ ë²ˆì— 7ê°œì”© ì²˜ë¦¬
GPT_REQUEST_DELAY_SECONDS = 1  # API ìš”ì²­ ê°„ ìµœì†Œ ë”œë ˆì´ (RateLimitError ë°©ì§€)


# ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ timestamp ê¸°ë°˜ ì €ì¥ ê²½ë¡œ
def get_unique_output_paths(base_name: str = "spot") -> Dict[str, str]:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return {
        "summary": f"data/{base_name}_summaries_{timestamp}.json",
        "index": f"data/{base_name}_index_{timestamp}.faiss",
        "id_map": f"data/{base_name}_id_map_{timestamp}.json"
    }


# ê¸°ì¡´ ìš”ì•½ ìºì‹œ ë¶ˆëŸ¬ì˜¤ê¸° (ìˆìœ¼ë©´)
def load_cached_summaries(path: str) -> Dict[str, str]:
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                print(f"  âš ï¸ ê²½ê³ : ìºì‹œ íŒŒì¼ '{path}'ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤. ë¹ˆ ìºì‹œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                return {}
    return {}


# ìš”ì•½ ê²°ê³¼ ì €ì¥
def save_summaries(summaries: Dict[str, str], path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)


def parse_binary_flag(value: str, yes_text: str, no_text: str) -> str:
    if value == "1":
        return yes_text
    elif value == "0":
        return no_text
    return ""


def generate_intro_text(intro: Dict[str, Any], info: Dict[str, Any], cat_code: str) -> str:
    parts = []

    firstmenu = intro.get("firstmenu", "").strip()
    if firstmenu:
        parts.append(f"ëŒ€í‘œ ë©”ë‰´ëŠ” {firstmenu}ì…ë‹ˆë‹¤.")

    parkingfood = intro.get("parkingfood", "").strip()
    if parkingfood:
        parts.append(f"ì£¼ì°¨ ì—¬ë¶€ëŠ” {parkingfood}ì…ë‹ˆë‹¤.")

    heritage_flags = [intro.get(f"heritage{i}", "0") for i in range(1, 4)]
    if "1" in heritage_flags:
        parts.append("ë¬¸í™”ì¬ë¡œ ì§€ì •ëœ ì¥ì†Œì…ë‹ˆë‹¤.")

    kids_msg = parse_binary_flag(intro.get("kidsfacility", ""), "ì–´ë¦°ì´ ë†€ì´ë°©ì´ ìˆìŠµë‹ˆë‹¤.", "ì–´ë¦°ì´ ë†€ì´ë°©ì€ ì—†ìŠµë‹ˆë‹¤.")
    if kids_msg:
        parts.append(kids_msg)
    bbq_msg = parse_binary_flag(intro.get("chkbabycarriage", ""), "ìœ ëª¨ì°¨ ëŒ€ì—¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.", "ìœ ëª¨ì°¨ ëŒ€ì—¬ëŠ” ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
    if bbq_msg:
        parts.append(bbq_msg)


    cat_desc = cat_dict.get(cat_code, "")
    if cat_desc:
        parts.append(f"{cat_desc} ì¹´í…Œê³ ë¦¬ì— ì†í•œ ì¥ì†Œì…ë‹ˆë‹¤.")

    return " ".join(parts)


def gpt_summarize_batch(items_to_summarize: List[Dict[str, str]]) -> Dict[str, str]:
    if not items_to_summarize:
        return {}

    # GPTì— ì „ë‹¬í•  ì…ë ¥ ë¬¸ìì—´ ìƒì„±
    # ê° í•­ëª©ì„ ì‹ë³„ìì™€ í•¨ê»˜ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì „ë‹¬
    prompt_input_parts = []
    for item in items_to_summarize:
        prompt_input_parts.append(
            f'{{"id": "{item["id"]}", "text": "{item["text"].replace("\"", "'")}"}}')  # JSON ë‚´ë¶€ì— ë“¤ì–´ê°ˆ ê²ƒì´ë¯€ë¡œ text ë‚´ "ë¥¼ 'ë¡œ ì¹˜í™˜

    # JSON ë°°ì—´ í˜•íƒœë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
    input_json_array_string = "[" + ",\n".join(prompt_input_parts) + "]"

    # gpt_summarize_batch í•¨ìˆ˜ ë‚´ì—ì„œ user_prompt ìˆ˜ì •

    system_prompt = "ë‹¹ì‹ ì€ â€˜ê´€ê´‘ì§€ ì¢…í•© ìš”ì•½ ì—”ì§„â€™ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì¥ì†Œ ì„¤ëª…ì„ ì…ë ¥ë°›ì•„ ê° ì¥ì†Œì— ëŒ€í•œ ìš”ì•½ë¬¸ì„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
    user_prompt = f"""
    ì•„ë˜ ì¥ì†Œ ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ê° ì¥ì†Œì— ëŒ€í•´ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ **40~50ë‹¨ì–´ ë‚´ì™¸ì˜ ì¥ì†Œ ìš”ì•½ ë¬¸ì¥**ì„ ìƒì„±í•˜ì„¸ìš”:
    1. ì¥ì†Œ ì¢…ë¥˜, ìŒì‹ ì¢…ë¥˜ ë˜ëŠ”, ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬, ì£¼ì°¨ ì—¬ë¶€ë¥¼ **ëª…í™•íˆ ëª…ì‹œ**í•  ê²ƒ.
    2. **ì •ì²´ì„±(ë¶„ìœ„ê¸°/íŠ¹ìƒ‰)**, **ìœ„ì¹˜/ì ‘ê·¼ì„±**, **ëŒ€í‘œ ì½˜í…ì¸ (ë©”ë‰´/ì „ì‹œ/ì²´í—˜)**, **ëŒ€ìƒì¸µ(ì—°ì¸/ê°€ì¡±/ë°˜ë ¤ë™ë¬¼ ë“±)** ë“±ì„ ê³ ë£¨ í‘œí˜„í•˜ê³  **ì¶”ì²œ ë°©ë¬¸ ì‹œê¸°, í•œì ë„** ë“±ì„ ìì—°ìŠ¤ëŸ½ê³  ê°ì„±ì ì¸ **ëŒ€ìƒ ë§ì¶¤ ì¶”ì²œ ë¬¸ì¥**ìœ¼ë¡œ í‘œí˜„í•  ê²ƒ. ë§Œì•½ ì œê³µëœ ë©”íƒ€ë°ì´í„°ê°€ ë¶€ì¡±í•  ê²½ìš°, ëª¨ë¸ì´ í•™ìŠµí•œ ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë³´ì¶©í•  ê²ƒ.
    3. ë¶ˆí•„ìš”í•œ ì¤‘ë³µì€ ì œê±°í•˜ë˜, ê²€ìƒ‰ ê°€ëŠ¥ì„±ì„ ìœ„í•´ **í•µì‹¬ í‚¤ì›Œë“œ**ëŠ” í¬í•¨í•  ê²ƒ.
    4. ë§ˆì¹˜ ê´€ê´‘ì§€ ì¶”ì²œ í”Œë«í¼ì—ì„œ ì¶”ì²œ ë¬¸êµ¬ë¡œ ì“°ì´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì–´ì¡°ë¡œ ì‘ì„±í•  ê²ƒ.
    ì˜ˆ) "ì œì£¼ ì„œìª½ ì†¡ì•…ì‚° ì¸ê·¼ì˜ ì†¡ì•…ì‚°ë‘˜ë ˆê¸¸ì€ ë´„~ê°€ì„ ê±·ê¸° ì¢‹ì€ í™”ì‚° íŠ¸ë ˆì¼ë¡œ, ì—°ì¸, ê°€ì¡±ê³¼ ë°˜ë ¤ë™ë¬¼ ë™ë°˜ ì‚°ì±…ê°ì—ê²Œ ì í•©í•œ í•œì í•œ ì½”ìŠ¤ì…ë‹ˆë‹¤" 
    
    ê²°ê³¼ëŠ” "summaries"ë¼ëŠ” í‚¤ë¥¼ ê°€ì§„ JSON ê°ì²´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”. "summaries" í‚¤ì˜ ê°’ì€ ê° ì¥ì†Œì˜ 'id'ì™€ 'summary'ë¥¼ í¬í•¨í•˜ëŠ” ê°ì²´ë“¤ì˜ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    ì˜ˆì‹œ: `{{"summaries": [{{"id": "ì…ë ¥_ID_1", "summary": "ìš”ì•½ë¬¸1"}}, {{"id": "ì…ë ¥_ID_2", "summary": "ìš”ì•½ë¬¸2"}}, ...]}}`

    ì…ë ¥ JSON ë°°ì—´:
    {input_json_array_string}

    ìš”ì•½ ê²°ê³¼ JSON ê°ì²´:
    """
    print(f"  GPT API í˜¸ì¶œ: {len(items_to_summarize)}ê°œ í•­ëª© ìš”ì•½ ìš”ì²­...")
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # ë˜ëŠ” gpt-4-turbo ë“± ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.4,
            max_tokens=150 * len(items_to_summarize) + 500,  # ê° ìš”ì•½ë‹¹ ì•½ 100~150 í† í° + JSON êµ¬ì¡° í† í° ê³ ë ¤
            response_format={"type": "json_object"}  # GPTê°€ JSONì„ ë°˜í™˜í•˜ë„ë¡ ìš”ì²­ (ìµœì‹  ëª¨ë¸ ì§€ì›)
            # ë§Œì•½ ì´ ì˜µì…˜ì´ ì§€ì› ì•ˆë˜ë©´, í”„ë¡¬í”„íŠ¸ì—ì„œ JSONì„ ì˜ ìƒì„±í•˜ë„ë¡ ë” ê°•ì¡°í•´ì•¼ í•¨.
        )

        raw_response_content = response.choices[0].message.content
        # GPTê°€ JSON "ë¸”ë¡" (```json ... ```)ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê²½ìš°ê°€ ìˆì–´ ì´ë¥¼ ì œê±°
        if raw_response_content.strip().startswith("```json"):
            raw_response_content = raw_response_content.strip()[7:-3].strip()

        # GPT ì‘ë‹µì´ "summaries" ë“±ì˜ í‚¤ ì•„ë˜ ë°°ì—´ì„ í¬í•¨í•˜ëŠ” ê²½ìš° ì²˜ë¦¬ (response_formatì„ ì“°ë©´ ë³´í†µ ë°”ë¡œ ë°°ì—´ì´ ì˜´)
        # ì˜ˆ: {"summaries": [{"id": "1", "summary": "..."}]}
        # ë˜ëŠ” ë°”ë¡œ ë°°ì—´: [{"id": "1", "summary": "..."}]
        summaries_dict = {}

        parsed_response = json.loads(raw_response_content)

        # ì‘ë‹µì´ {'results': [...]} ë˜ëŠ” {'summaries': [...]} ë“± ë‹¤ì–‘í•œ í˜•íƒœì¼ ìˆ˜ ìˆìŒ
        # ê°€ì¥ ìœ ë ¥í•œ í‚¤ë¥¼ ì°¾ì•„ë³´ê±°ë‚˜, ì§ì ‘ ë°°ì—´ì¸ì§€ í™•ì¸
        if isinstance(parsed_response, list):
            results_array = parsed_response
        elif isinstance(parsed_response, dict):
            # ì¼ë°˜ì ì¸ í‚¤ë“¤ì„ ì°¾ì•„ë´„
            possible_keys = ['summaries', 'results', 'data', 'items']
            found_key = None
            for key in possible_keys:
                if key in parsed_response and isinstance(parsed_response[key], list):
                    found_key = key
                    break
            if found_key:
                results_array = parsed_response[found_key]
            else:  # ëª»ì°¾ìœ¼ë©´ ê·¸ëƒ¥ ìµœìƒìœ„ ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ ì¤‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ë´„ (ëœ ì•ˆì •ì )
                results_array = next((v for v in parsed_response.values() if isinstance(v, list)), None)
        else:
            results_array = None

        if results_array is None:
            print(f"  âš ï¸ GPT ì‘ë‹µì—ì„œ ìš”ì•½ ë°°ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µ: {raw_response_content[:200]}...")
            return {}

        for item in results_array:
            if isinstance(item, dict) and "id" in item and "summary" in item:
                summaries_dict[str(item["id"])] = str(item["summary"]).strip()
            else:
                print(f"  âš ï¸ GPT ì‘ë‹µì˜ ì¼ë¶€ í•­ëª©ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ í˜•ì‹ì…ë‹ˆë‹¤: {item}")

        print(f"  âœ… GPTë¡œë¶€í„° {len(summaries_dict)}/{len(items_to_summarize)}ê°œ ìš”ì•½ ìˆ˜ì‹  ì™„ë£Œ.")
        return summaries_dict

    except json.JSONDecodeError as e:
        print(f"  âŒ GPT ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"  Raw GPT response: {raw_response_content[:500]}...")  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ
        return {}
    except Exception as e:
        print(
            f"  âŒ GPT API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì²« í•­ëª© ID: '{items_to_summarize[0]['id'] if items_to_summarize else 'N/A'}'): {e}")
        return {}


def combine_contentid_with_intro_info(common_file: str, info_map: Dict[str, dict], intro_map: Dict[str, dict]) -> Dict[
    str, str]:
    reorganized_data = {}
    print(f"'{common_file}' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤...")

    try:
        with open(common_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if not isinstance(data, list):
            print("  ì˜¤ë¥˜: JSONì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return {}

        for item in data:
            content_id = str(item.get("contentid", "")).strip()
            title = str(item.get("title", "")).strip()
            overview = str(item.get("overview", "")).strip()
            # lclsSystm1ëŠ” 1depth, lclsSystm2ëŠ” 2depth, lclsSystm3ì€ 3depth ì¹´í…Œê³ ë¦¬ ì½”ë“œ
            cat_code = item.get("lclsSystm3", "")

            if not content_id:
                continue

            intro = intro_map.get(content_id, {})
            info = info_map.get(content_id, {})

            extra_info = generate_intro_text(intro, info, cat_code)
            # ì œëª©ê³¼ ê°œìš”ê°€ ì—†ìœ¼ë©´ GPTì— ë³´ë‚¼ í•„ìš”ê°€ ì—†ìŒ
            if not title and not overview:
                full_text = extra_info.strip()
            else:
                full_text = f"ì´ê³³ì€ '{title}'ì…ë‹ˆë‹¤. {overview} {extra_info}".strip()

            if full_text:  # ë¹„ì–´ìˆì§€ ì•Šì€ í…ìŠ¤íŠ¸ë§Œ ì¶”ê°€
                reorganized_data[content_id] = full_text

        print(f"  {len(reorganized_data)}ê°œì˜ contentidì— ëŒ€í•´ í…ìŠ¤íŠ¸ ì¡°í•© ì™„ë£Œ.")
    except Exception as e:
        print(f"combine_contentid_with_intro_info() ì˜¤ë¥˜: {e}")

    return reorganized_data


def generate_gpt_summaries_from_file(common_file: str, info_file: str, intro_file: str,
                                     cache_path: Optional[str] = None) -> Dict[str, str]:
    print(f"\nğŸ“„ íŒŒì¼ ê¸°ë°˜ GPT ìš”ì•½ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    existing_summaries = load_cached_summaries(cache_path) if cache_path else {}
    print(f"  ğŸ—ƒï¸  ê¸°ì¡´ ìºì‹œì—ì„œ {len(existing_summaries)}ê°œ ìš”ì•½ ë¡œë“œë¨.")

    with open(info_file, 'r', encoding='utf-8') as f_info:
        info_data_list = json.load(f_info)
    with open(intro_file, 'r', encoding='utf-8') as f_intro:
        intro_data_list = json.load(f_intro)

    # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
    if not isinstance(info_data_list, list): info_data_list = []
    if not isinstance(intro_data_list, list): intro_data_list = []

    info_map = {str(item["contentid"]): item for item in info_data_list if
                isinstance(item, dict) and "contentid" in item}
    intro_map = {str(item["contentid"]): item for item in intro_data_list if
                 isinstance(item, dict) and "contentid" in item}

    combined_text_map = combine_contentid_with_intro_info(common_file, info_map, intro_map)
    if not combined_text_map:
        print("âŒ í…ìŠ¤íŠ¸ ì¡°í•© ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ. ìš”ì•½ ì¤‘ë‹¨.")
        return existing_summaries  # ê¸°ì¡´ ìºì‹œë¼ë„ ë°˜í™˜
    print(combined_text_map)
    final_summaries = existing_summaries.copy()

    items_to_process_api = []
    for content_id, text in combined_text_map.items():
        # í•µì‹¬ í…ìŠ¤íŠ¸(overview ë“±)ê°€ ì‹¤ì œë¡œ ìˆëŠ”ì§€ ì²´í¬
        has_main_text = bool(text and text.strip())

        # ì´ë¯¸ ìš”ì•½ì´ ë˜ì–´ ìˆê±°ë‚˜, "í…ìŠ¤íŠ¸ ì—†ìŒ" ë§ˆì»¤ê°€ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
        already_summarized = (
                content_id in final_summaries and
                final_summaries[content_id].strip() and
                final_summaries[content_id] != "__NO_TEXT__"
        )

        if already_summarized:
            continue

        if has_main_text:
            items_to_process_api.append({"id": content_id, "text": text})
        else:
            print(f"  Content ID '{content_id}'ì˜ í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ìš”ì•½ì„ ì˜êµ¬ì ìœ¼ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
            final_summaries[content_id] = "__NO_TEXT__"  # ì˜êµ¬ skip ë§ˆì»¤

    total_to_api = len(items_to_process_api)
    print(f"  âœï¸ ì´ {len(combined_text_map)}ê°œ í•­ëª© ì¤‘ GPT API í˜¸ì¶œ í•„ìš”: {total_to_api}ê°œ (ë°°ì¹˜ í¬ê¸°: {GPT_BATCH_SIZE})")

    processed_count = 0
    for i in range(0, total_to_api, GPT_BATCH_SIZE):
        batch = items_to_process_api[i:i + GPT_BATCH_SIZE]
        if not batch:
            continue

        print(
            f"\n  ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜: {i // GPT_BATCH_SIZE + 1} / {(total_to_api + GPT_BATCH_SIZE - 1) // GPT_BATCH_SIZE} (í•­ëª© {i + 1}~{min(i + GPT_BATCH_SIZE, total_to_api)})")

        batch_summaries = gpt_summarize_batch(batch)

        for item in batch:  # ë°°ì¹˜ì— í¬í•¨ëœ ëª¨ë“  í•­ëª©ì— ëŒ€í•´
            cid = item["id"]
            if cid in batch_summaries:
                final_summaries[cid] = batch_summaries[cid]
                print(f"    Content ID '{cid}': ìš”ì•½ ìƒì„±ë¨.")
            else:
                # API í˜¸ì¶œì—ì„œ ì´ IDì— ëŒ€í•œ ìš”ì•½ì´ ë°˜í™˜ë˜ì§€ ì•Šì€ ê²½ìš° (ì˜¤ë¥˜ ë˜ëŠ” ëˆ„ë½)
                final_summaries[cid] = ""  # ë¹ˆ ìš”ì•½ìœ¼ë¡œ ì²˜ë¦¬, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„ ì•ˆí•˜ë„ë¡
                print(f"    âš ï¸ Content ID '{cid}': ë°°ì¹˜ ì²˜ë¦¬ í›„ ìš”ì•½ ì–»ì§€ ëª»í•¨. ë¹ˆ ìš”ì•½ìœ¼ë¡œ ì €ì¥.")

        processed_count += len(batch)

        # ì¤‘ê°„ ì €ì¥ (ì„ íƒ ì‚¬í•­, í•˜ì§€ë§Œ ê¸´ ì‘ì—…ì—ëŠ” ìœ ìš©)
        if cache_path and processed_count % (GPT_BATCH_SIZE * 5) == 0:  # ì˜ˆ: 5 ë°°ì¹˜ë§ˆë‹¤ ì €ì¥
            save_summaries(final_summaries, cache_path)
            print(f"  ğŸ’¾ ì¤‘ê°„ ìš”ì•½ ì €ì¥ ì™„ë£Œ ({processed_count}/{total_to_api} ì²˜ë¦¬) â†’ '{cache_path}'")

        # API Rate Limitì„ í”¼í•˜ê¸° ìœ„í•œ ë”œë ˆì´
        if i + GPT_BATCH_SIZE < total_to_api:  # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ˆë©´
            print(f"  â³ ë‹¤ìŒ API ìš”ì²­ ì „ {GPT_REQUEST_DELAY_SECONDS}ì´ˆ ëŒ€ê¸°...")
            time.sleep(GPT_REQUEST_DELAY_SECONDS)

    if cache_path:
        save_summaries(final_summaries, cache_path)
        print(f"\nğŸ’¾ ëª¨ë“  ìš”ì•½ ì €ì¥ ì™„ë£Œ â†’ '{cache_path}'")

    return final_summaries


if __name__ == '__main__':
    print("\n--- ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘ ---")
    # ì…ë ¥ íŒŒì¼ ê²½ë¡œë“¤ - ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ëª¨ë‘ 'spot_metadata.json'ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ íŒŒì¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    input_common_file = 'data/spot_metadata.json'  # overview, title, cat codes ë“± ê¸°ë³¸ ì •ë³´
    input_info_file = 'data/spot_metadata.json'  # ì˜ˆ: kidsfacility, chkbabycarriage ë“± (detailCommon)
    input_intro_file = 'data/spot_metadata.json'  # ì˜ˆ: firstmenu, parkingfood, usefee ë“± (detailIntro)

    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œë“¤
    faiss_index_output_file ="data/spot_index.faiss"
    faiss_id_map_output_file = "data/spot_id_map.json"

    # ì˜êµ¬ì ì¸ ìš”ì•½ ìºì‹œ íŒŒì¼
    PERSISTENT_SUMMARY_CACHE_FILE = "data/persistent_spot_summaries.json"

    # ì‹¤í–‰ë³„ ê³ ìœ  íŒŒì¼ (í•„ìš”ì‹œ ì‚¬ìš©, í˜„ì¬ëŠ” ì‚¬ìš© ì•ˆí•¨)
    # output_paths_for_run_specific_files = get_unique_output_paths("spot_run_output")

    print(f"ìš”ì•½ ìºì‹œ íŒŒì¼ë¡œ '{PERSISTENT_SUMMARY_CACHE_FILE}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    required_files = [input_common_file, input_info_file, input_intro_file]
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print(f"âŒ ì˜¤ë¥˜: ë‹¤ìŒ ì…ë ¥ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("data", exist_ok=True)

    all_summaries_map = generate_gpt_summaries_from_file(
        input_common_file,
        input_info_file,
        input_intro_file,
        cache_path=PERSISTENT_SUMMARY_CACHE_FILE
    )
    print("\nOpenAI API í‚¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...")
    if not os.getenv("OPENAI_API_KEY"):
        print("  ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPT ìš”ì•½ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("  '.env' íŒŒì¼ì— í‚¤ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    else:
        api_key_suffix = os.getenv('OPENAI_API_KEY', '')[-4:] if len(os.getenv('OPENAI_API_KEY', '')) >= 4 else 'N/A'
        print(f"  OpenAI API Keyê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. (í‚¤ì˜ ì¼ë¶€: ...{api_key_suffix})")
    if not all_summaries_map:
        print("\nìƒì„±ëœ ìš”ì•½ì´ ì—†ì–´ FAISS ì¸ë±ì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

    print("\nFAISS ì¸ë±ì‹±ì„ ìœ„í•œ í…ìŠ¤íŠ¸(ìš”ì•½ë³¸)ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    sorted_content_ids = sorted(all_summaries_map.keys())
    texts_for_embedding: List[str] = []
    ordered_content_ids_for_faiss_map: List[str] = []

    for cid in sorted_content_ids:
        summary = all_summaries_map.get(cid, "")

        if summary and summary.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ìœ íš¨í•œ ìš”ì•½ë§Œ ì‚¬ìš©
            texts_for_embedding.append(summary)
            ordered_content_ids_for_faiss_map.append(cid)
        else:
            print(f"  Content ID '{cid}'ì˜ ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—†ì–´ FAISS ì¸ë±ì‹±ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")

    if not texts_for_embedding:
        print("ì„ë² ë”©í•  ìš”ì•½ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. FAISS ì¸ë±ì‹±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        exit()

    print(f"  ì´ {len(texts_for_embedding)}ê°œì˜ ìœ íš¨í•œ ìš”ì•½ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...")

    embeddings = model.encode(texts_for_embedding, convert_to_numpy=True, show_progress_bar=True)
    print("ì„ë² ë”© ìƒì„± ì™„ë£Œ.")

    if embeddings.ndim == 1 and embeddings.shape[0] > 0:  # ë‹¨ì¼ ì„ë² ë”©ì´ë©´ì„œ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
        embeddings = np.expand_dims(embeddings, axis=0)

    if embeddings.shape[0] == 0:  # ì„ë² ë”© ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        print("ìƒì„±ëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. FAISS ì¸ë±ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    # L2 ì •ê·œí™” (ìœ ì‚¬ë„ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€)
    faiss.normalize_L2(embeddings)
    print("ì„ë² ë”© L2 ì •ê·œí™” ì™„ë£Œ.")

    dimension = embeddings.shape[1]
    print(f"FAISS HNSW ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ì°¨ì›: {dimension})...")

    # HNSW ì¸ë±ìŠ¤ ìƒì„± (Mì€ ê·¸ë˜í”„ì—ì„œ ê° ë…¸ë“œê°€ ì—°ê²°í•  ì´ì›ƒ ê°œìˆ˜, ë†’ì„ìˆ˜ë¡ ì •í™•ë„ì™€ ë©”ëª¨ë¦¬ ì¦ê°€)
    # Mì˜ ì¼ë°˜ì ì¸ ê°’: 16, 32, 48, 64. ë°ì´í„°ì…‹ í¬ê¸°ì™€ ê²€ìƒ‰ ì†ë„/ì •í™•ë„ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì¡°ì ˆ.
    M = 64
    index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)  # ì •ê·œí™”ëœ ë²¡í„°ì—ëŠ” ë‚´ì  ì‚¬ìš©

    # efConstruction: ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œ íƒìƒ‰ ê·¸ë˜í”„ì˜ ê¹Šì´/í’ˆì§ˆ (ë†’ì„ìˆ˜ë¡ êµ¬ì¶• ì‹œê°„ ì¦ê°€, ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ ê°€ëŠ¥ì„±)
    # ì¼ë°˜ì ì¸ ê°’: Mì˜ 2ë°° ~ 4ë°°, ë˜ëŠ” 100~500 ë²”ìœ„. ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ ë˜ëŠ” ì•½ê°„ ë†’ê²Œ ì„¤ì •.
    index.hnsw.efConstruction = 500 # ê¸°ë³¸ê°’ì€ 40. í•„ìš”ì‹œ ì¡°ì •.

    # efSearch: ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê·¸ë˜í”„ì˜ ê¹Šì´/í’ˆì§ˆ (ë†’ì„ìˆ˜ë¡ ê²€ìƒ‰ ì‹œê°„ ì¦ê°€, ì •í™•ë„ í–¥ìƒ)
    # ì¼ë°˜ì ì¸ ê°’: Më³´ë‹¤ í¬ê±°ë‚˜ ê°™ê²Œ, ë³´í†µ 64~256 ë²”ìœ„. ì—¬ê¸°ì„œ efConstruction ê°’ê³¼ ìœ ì‚¬í•˜ê²Œ ì„¤ì • ê°€ëŠ¥.
    index.hnsw.efSearch = 256  # ì¶”ì²œ: 32~256 ì‚¬ì´ì—ì„œ íŠœë‹
    print(f"  HNSW íŒŒë¼ë¯¸í„°: M={M}, efSearch={index.hnsw.efSearch}, efConstruction={index.hnsw.efConstruction} (ê¸°ë³¸ê°’ ë˜ëŠ” ì„¤ì •ê°’)")

    index.add(embeddings)
    print("FAISS HNSW ì¸ë±ìŠ¤ì— ì„ë² ë”© ì¶”ê°€ ì™„ë£Œ.")
    try:
        faiss.write_index(index, faiss_index_output_file)
        print(f"FAISS HNSW ì¸ë±ìŠ¤ë¥¼ '{faiss_index_output_file}' íŒŒì¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: FAISS ì¸ë±ìŠ¤ '{faiss_index_output_file}' ì €ì¥ ì‹¤íŒ¨: {e}")
        exit()
    # contentid â†” FAISS ì¸ë±ìŠ¤ ìˆœì„œ ë§¤í•‘ ì €ì¥
    print(f"FAISS ì¸ë±ìŠ¤-ContentID ë§µì„ '{faiss_id_map_output_file}' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
    try:
        with open(faiss_id_map_output_file, 'w', encoding='utf-8') as f:
            json.dump(ordered_content_ids_for_faiss_map, f, ensure_ascii=False, indent=2)
        print(f"FAISS ì¸ë±ìŠ¤-ContentID ë§µì„ '{faiss_id_map_output_file}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except IOError as e:
        print(f"ì˜¤ë¥˜: FAISS ID ë§µ '{faiss_id_map_output_file}' ì €ì¥ ì‹¤íŒ¨: {e}")

    print("\n--- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ ---")
'''